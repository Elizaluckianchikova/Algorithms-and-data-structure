{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRFAgstCaOXkKUJZUyFfhf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Elizaluckianchikova/Algorithms-and-data-structure/blob/main/gradient_down.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Градиентный спуск**\n",
        "\n",
        "**Градиентный спуск**- это метод оптимизации функции, который заключается в движении в направлении антиградиента функции. Градиент указывает направление наибольшего возрастания функции, а антиградиент - направление наибольшего убывания функции.\n",
        "\n",
        "Для использования градиентного спуска необходимо знать градиент функции, то есть вектор значений, показывающий направление и величину изменения функции при изменении каждого аргумента. Градиент можно найти, используя производные функции.\n",
        "\n",
        "Процесс градиентного спуска состоит из следующих шагов:\n",
        "\n",
        "– Определить начальные значения аргументов функции.\n",
        "– Вычислить градиент функции в этих начальных точках.\n",
        "– Найти направление антиградиента и сделать шаг в этом направлении.\n",
        "– Повторить шаги 2 и 3 до тех пор, пока функция не достигнет минимума или заданного значения точности.\n",
        ". В контексте машинного обучения, градиентный спуск часто используется для обновления параметров модели с целью минимизации функции потерь."
      ],
      "metadata": {
        "id": "qHXEgeegmm1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from future import annotations\n",
        "\n",
        "def gradient_descent(f, x0, alpha, beta, max_iter):\n",
        "  x = x0\n",
        "  for _ in range(max_iter):\n",
        "    grad_x = gradient(f, x)\n",
        "    if grad_x == 0:\n",
        "      return x\n",
        "    x -= alpha * grad_x\n",
        "    if random.random() < beta:\n",
        "      x += (1 - 2 * beta) * (x - x0)\n",
        "      return x\n",
        "\n",
        "#Test function\n",
        "def f(x):\n",
        "  return (x[0] - 1)**2 + (x[1] - 5)**2\n",
        "  x0 = (3, 3)\n",
        "  alpha = 0.01\n",
        "  beta = 0.5\n",
        "  max_iter = 1000\n",
        "  result = gradient_descent(f, x0, alpha, beta, max_iter)\n",
        "print(f\"Result: {result}\")"
      ],
      "metadata": {
        "id": "8fIkT7VHnpsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Функция, для которой мы хотим найти минимум\n",
        "def loss_function(x):\n",
        "    return (x-5)**2\n",
        "\n",
        "# Производная функции потерь\n",
        "def gradient(x):\n",
        "    return 2*(x-5)\n",
        "\n",
        "# Начальное значение параметра\n",
        "x = 0\n",
        "\n",
        "# Скорость обучения (learning rate)\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Количество итераций\n",
        "num_iterations = 100\n",
        "\n",
        "# Градиентный спуск\n",
        "for i in range(num_iterations):\n",
        "    grad = gradient(x)\n",
        "    x = x - learning_rate * grad\n",
        "    print(f'Iteration {i+1}: x = {x}, loss = {loss_function(x)}')\n",
        "\n",
        "print(f'Минимум достигается в точке x = {x}')\n"
      ],
      "metadata": {
        "id": "IV6LnEiapy_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # Подключаем NumPy для работы с массивами\n",
        "\n",
        "def f(x): # Определяем сигмоидную функцию активации\n",
        "    return 2/(1 + np.exp(-x)) — 1\n",
        "\n",
        "def df(x): # Рассчитываем производную сигмоидной функции\n",
        "    return 0.5 * (1 + x) * (1 — x)\n",
        "\n",
        "W1 = np.array([[-0.2, 0.3, -0.4], [0.1, -0.3, -0.4]])\n",
        "W2 = np.array([0.2, 0.3])\n",
        "\n",
        "def go_forward(inp):\n",
        "    sum = np.dot(W1, inp)\n",
        "    out = np.array([f(x) for x in sum])\n",
        "\n",
        "    sum = np.dot(W2, out)\n",
        "    y = f(sum)\n",
        "    return (y, out)\n",
        "\n",
        "def train(epoch):\n",
        "    global W2, W1\n",
        "    lmd = 0.001\n",
        "    N = 100000\n",
        "    count = len(epoch)\n",
        "    for k in range(N):\n",
        "        x = epoch[np.random.randint(0, count)]\n",
        "        y, out = go_forward(x[0:3])\n",
        "        e = y — x[-1]\n",
        "        delta = e * df(y)\n",
        "        W2[0] = W2[0] — lmd * delta * out[0]\n",
        "        W2[1] = W2[1] — lmd * delta * out[1]\n",
        "\n",
        "        delta2 = W2 * delta * df(out)\n",
        "\n",
        "        W1[0, :] = W1[0, :] — np.array(x[0:3]) * delta2[0] * lmd\n",
        "        W1[1, :] = W1[1, :] — np.array(x[0:3]) * delta2[1] * lmd\n",
        "\n",
        "epoch = [(-1, -1, -1, -1),\n",
        "         (-1, -1, 1, 1),\n",
        "          (1, 1, -1, -1),\n",
        "         (-1, 1, -1, -1),\n",
        "         (-1, 1, 1, -1),\n",
        "         (1, -1, -1, -1),\n",
        "         (1, -1, 1, 1),\n",
        "         (1, 1, 1, -1)]\n",
        "train(epoch)\n",
        "\n",
        "for x in epoch:\n",
        "    y, out = go_forward(x[0:3])\n",
        "    print(f'Выходное значение нейронной сети: {y} => {x[-1]}')"
      ],
      "metadata": {
        "id": "YNomprt9q64D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}